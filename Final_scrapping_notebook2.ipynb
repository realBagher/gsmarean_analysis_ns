{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# library"
      ],
      "metadata": {
        "id": "TyBq2Pb7ffdr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdCUVhvrfcNr"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Tuple, Iterator, Any\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "import time\n",
        "import threading\n",
        "import concurrent.futures\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "V4vRI9oHfmCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scrapping"
      ],
      "metadata": {
        "id": "sHyCDJWNfjeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "BASE_URL: str = 'https://www.gsmarena.com'\n",
        "\n",
        "HEADERS: Dict[str, str] = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 '\n",
        "                  'Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,'\n",
        "              'application/signed-exchange;v=b3;q=0.7',\n",
        "    'Accept-Language': 'en-US,en;q=0.9'\n",
        "}"
      ],
      "metadata": {
        "id": "JroABr5XfrPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def to make a request\n",
        "def make_request(url:str, headers:Dict[str, str]) -> requests.Response:\n",
        "    try:\n",
        "        time.sleep(3)\n",
        "        response = requests.get(url, timeout=50, headers=headers)\n",
        "        response.raise_for_status()  # Raises a HTTPError if the response status code is 4XX/5XX\n",
        "        return response\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error while making a request: {str(e)}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "xDGF_XcNfwH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get all brands phone we need\n",
        "\n",
        "def get_all_brands(content: BeautifulSoup) -> Dict[str, str]:\n",
        "    brands: Dict[str, str] = {}\n",
        "    for item in content.select('.brandmenu-v2.light.l-box.clearfix ul li'):\n",
        "        brand_name = item.text.upper()\n",
        "        if brand_name in ['ALCATEL', 'APPLE', 'ASUS', 'BLU', 'HTC', 'HUAWEI', 'INFINIX', 'LENOVO', 'LG', 'NOKIA', 'SONY', 'XIAOMI', 'ZTE', 'SAMSUNG']:\n",
        "            brands[brand_name] = item.select_one('a')['href']\n",
        "    return brands"
      ],
      "metadata": {
        "id": "oDC2bxoDgQ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all detail from each phone\n",
        "\n",
        "def fetch_phone_data( phone_url:str ) -> Dict[str, Any]:\n",
        "    full_url = f\"{BASE_URL}/{phone_url}\"\n",
        "    response = make_request(full_url, HEADERS)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # phone name\n",
        "        name = soup.select_one('[data-spec=\"modelname\"]').text if soup.select_one('[data-spec=\"modelname\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        # network\n",
        "        network=soup.select_one('[data-spec=\"nettech\"]').text if soup.select_one('[data-spec=\"nettech\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #launch\n",
        "        announced_text = soup.select_one('[data-spec=\"year\"]').text if soup.select_one('[data-spec=\"year\"]') else None\n",
        "        if announced_text:\n",
        "            match = re.search(r'(\\d{4}), (\\w+)', announced_text)\n",
        "            if match:\n",
        "                year, month = match.groups()\n",
        "                try:\n",
        "                    announcedDate = datetime.strptime(f'{year} {month}', '%Y %B').strftime('%Y-%m')\n",
        "                except ValueError:\n",
        "                    announcedDate = 'Unknown'\n",
        "            else :\n",
        "                announcedDate = 'Unknown'\n",
        "\n",
        "        status_text = soup.find('td', {'data-spec': 'status'}).text if soup.find('td', {'data-spec': 'status'}) else None\n",
        "\n",
        "        releaseDate = 'Unknown'\n",
        "        try :\n",
        "            if status_text and \"Released\" in status_text:\n",
        "                release_date_text = status_text.split(\"Released\")[1].strip()\n",
        "                releaseDate = datetime.strptime(release_date_text, '%Y, %B %d').strftime('%Y-%m')\n",
        "            elif announced_text and \"Released\" in announced_text:\n",
        "                release_date_text = announced_text.split(\"Released\")[1].strip()\n",
        "                releaseDate = datetime.strptime(release_date_text, '%Y, %B %d').strftime('%Y-%m')\n",
        "        except ValueError:\n",
        "            releaseDate = 'Unknown'\n",
        "        availability=soup.select_one('[data-spec=\"status\"]').text.split(\".\")[0] if soup.select_one('[data-spec=\"status\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #body\n",
        "        dimensions = soup.select_one('[data-spec=\"dimensions\"]').text.split(\"mm\")[0] if soup.select_one('[data-spec=\"dimensions\"]') else 'Unknown'\n",
        "        weight = soup.select_one('[data-spec=\"weight\"]').text.split('g')[0] if soup.select_one('[data-spec=\"weight\"]') else 'Unknown'\n",
        "        build = soup.select_one('[data-spec=\"build\"]').text.split(\",\") if soup.select_one('[data-spec=\"build\"]') else \"unknown\"\n",
        "        sim = soup.select_one('[data-spec=\"sim\"]').text.split(\",\") if soup.select_one('[data-spec=\"sim\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #display\n",
        "        display_type = soup.select_one('[data-spec=\"displaytype\"]').text if soup.select_one('[data-spec=\"displaytype\"]') else 'Unknown'\n",
        "        display_size_text = soup.select_one('[data-spec=\"displaysize\"]').text if soup.select_one('[data-spec=\"displaysize\"]') else 'Unknown'\n",
        "        displaysizeI = 'Unknown'\n",
        "        displaysizeCM = 'Unknown'\n",
        "        displayratio = 'Unknown'\n",
        "        if display_size_text != 'Unknown':\n",
        "            parts = display_size_text.split(', ')\n",
        "            if len(parts) >= 1:\n",
        "                displaysizeI = parts[0].split(' inches')[0]\n",
        "            if len(parts) >= 2:\n",
        "                displaysizeCM = parts[1].split(' cm')[0]\n",
        "                ratio_part = parts[1].split('(')[-1] if '(' in parts[1] else 'Unknown'\n",
        "                displayratio = ratio_part.replace(')', '').strip() if ')' in ratio_part else 'Unknown'\n",
        "\n",
        "        resolution = soup.select_one('[data-spec=\"displayresolution\"]').text if soup.select_one('[data-spec=\"displayresolution\"]') else 'Unknown'\n",
        "        protection = soup.select_one('[data-spec=\"displayprotection\"]').text if soup.select_one('[data-spec=\"displayprotection\"]') else 'Unknown'\n",
        "        other_features = soup.select_one('[data-spec=\"displayother\"]').text if soup.select_one('[data-spec=\"displayother\"]') else 'Unknown'\n",
        "        '''\n",
        "        display_data = {\n",
        "            'Display_Type': display_type,\n",
        "            'Display_Size_Inches': displaysizeI,\n",
        "            'Display_Size_CM': displaysizeCM,\n",
        "            'Display_Ratio' : displayratio,\n",
        "            'Resolution': resolution,\n",
        "            'Protection': protection,\n",
        "            'Other_Features': other_features\n",
        "        }\n",
        "        '''\n",
        "\n",
        "        #os\n",
        "        os_text = soup.select_one('[data-spec=\"os\"]').text if soup.select_one('[data-spec=\"os\"]') else 'Unknown'\n",
        "        ostype = 'Unknown'\n",
        "        osversion = 'Unknown'\n",
        "        chipset = 'Unknown'\n",
        "        cpu = 'Unknown'\n",
        "        if os_text != 'Unknown':\n",
        "            os_parts = os_text.split(\",\")[0].split()\n",
        "            if len(os_parts) > 1:\n",
        "                ostype = os_parts[0]\n",
        "                osversion = ' '.join(os_parts[1:])\n",
        "        chipset = soup.select_one('[data-spec=\"chipset\"]').text if soup.select_one('[data-spec=\"chipset\"]') else 'Unknown'\n",
        "        cpu_text = soup.select_one('[data-spec=\"cpu\"]').text if soup.select_one('[data-spec=\"cpu\"]') else 'Unknown'\n",
        "        cpu = cpu_text.split(\" (\")[0] if \"(\" in cpu_text else cpu_text\n",
        "        gpu = soup.select_one('[data-spec=\"gpu\"]').text if soup.select_one('[data-spec=\"gpu\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #memory\n",
        "        card_slot = soup.select_one('[data-spec=\"memoryslot\"]').get_text(strip=True) if soup.select_one('[data-spec=\"memoryslot\"]') else 'Unknown'\n",
        "        internal_memory_element = soup.select_one('[data-spec=\"internalmemory\"]')\n",
        "        if internal_memory_element:\n",
        "            # Splitting by comma to handle multiple\n",
        "            internal_memory = internal_memory_element.get_text(strip=True).split(\", \")\n",
        "        else:\n",
        "            internal_memory = 'Unknown'\n",
        "        memory_other = soup.select_one('[data-spec=\"memoryother\"]').get_text(strip=True) if soup.select_one('[data-spec=\"memoryother\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #main camera\n",
        "        mainCam = soup.select_one('[data-spec=\"cam1modules\"]').get_text(separator=\"|\") if soup.select_one('[data-spec=\"cam1modules\"]') else 'Unknown'\n",
        "        ##separator to clearly between different camera specifications\n",
        "        mainCamNum = len(mainCam.split(\"|\")) if mainCam != 'Unknown' else 0\n",
        "        mainCamFeatures = soup.select_one('[data-spec=\"cam1features\"]').get_text().split(\", \") if soup.select_one('[data-spec=\"cam1features\"]') else 'Unknown'\n",
        "        mainCamVideo = soup.select_one('[data-spec=\"cam1video\"]').get_text().split(\", \") if soup.select_one('[data-spec=\"cam1video\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #selfie cam\n",
        "        selfieCam = soup.select_one('[data-spec=\"cam2modules\"]').get_text(separator=\"|\") if soup.select_one('[data-spec=\"cam2modules\"]') else 'Unknown'\n",
        "        selfieCamNum = selfieCam.count(\"MP\")\n",
        "        selfieCamFeatures = soup.select_one('[data-spec=\"cam2features\"]').get_text(separator=\", \").split(\", \") if soup.select_one('[data-spec=\"cam2features\"]') else \"Unknown\"\n",
        "        selfieCamVideo = soup.select_one('[data-spec=\"cam2video\"]').get_text(separator=\", \").split(\", \") if soup.select_one('[data-spec=\"cam2video\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #sound\n",
        "        #loudspeaker_info = soup.find('a', href=\"glossary.php3?term=loudspeaker\").find_next_sibling('td')\n",
        "        #loudspeaker = loudspeaker_info.get_text(strip=True) if loudspeaker_info else 'Unknown'\n",
        "        #jack_info = soup.find('a', href=\"glossary.php3?term=audio-jack\").find_next_sibling('td')\n",
        "        #jack = jack_info.get_text(strip=True) if jack_info else 'Unknown'\n",
        "        def find_feature_info(feature_label):\n",
        "            feature_info = \"Unknown\"  # Default value\n",
        "            for td in soup.find_all('td'):\n",
        "                if feature_label.lower() in td.get_text().lower():\n",
        "                    next_td = td.find_next_sibling('td')\n",
        "                    if next_td:\n",
        "                        feature_info = next_td.get_text(strip=True)\n",
        "                        break\n",
        "            return feature_info\n",
        "\n",
        "        loudspeaker = find_feature_info(\"Loudspeaker\")\n",
        "        jack = find_feature_info(\"3.5mm jack\")\n",
        "\n",
        "\n",
        "        #comms\n",
        "        wlan = soup.select_one('[data-spec=\"wlan\"]').get_text(strip=True) if soup.select_one('[data-spec=\"wlan\"]') else 'Unknown'\n",
        "        bluetooth = soup.select_one('[data-spec=\"bluetooth\"]').get_text(strip=True) if soup.select_one('[data-spec=\"bluetooth\"]') else 'Unknown'\n",
        "        gps = soup.select_one('[data-spec=\"gps\"]').get_text(strip=True) if soup.select_one('[data-spec=\"gps\"]') else 'Unknown'\n",
        "        nfc = soup.select_one('[data-spec=\"nfc\"]').get_text(strip=True) if soup.select_one('[data-spec=\"nfc\"]') else 'Unknown'\n",
        "        radio = soup.select_one('[data-spec=\"radio\"]').get_text(strip=True) if soup.select_one('[data-spec=\"radio\"]') else 'Unknown'\n",
        "        usb = soup.select_one('[data-spec=\"usb\"]').get_text(strip=True) if soup.select_one('[data-spec=\"usb\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #features\n",
        "        sensors = soup.select_one('[data-spec=\"sensors\"]').get_text(strip=True).split(\",\") if soup.select_one('[data-spec=\"sensors\"]') else 'Unknown'\n",
        "        features_other = soup.select_one('[data-spec=\"featuresother\"]').get_text(separator=\"|\").split(\"|\") if soup.select_one('[data-spec=\"featuresother\"]') else 'Unknown'\n",
        "\n",
        "\n",
        "        #battery\n",
        "        batteryType = soup.select_one('[data-spec=\"batdescription1\"]').get_text(strip=True) if soup.select_one('[data-spec=\"batdescription1\"]') else 'Unknown'\n",
        "        charging_info_element = soup.find('td', text=lambda x: x and \"Charging\" in x)\n",
        "        charging_info = charging_info_element.find_next_sibling('td') if charging_info_element else None\n",
        "        charging = ' '.join(charging_info.stripped_strings) if charging_info else 'Unknown'\n",
        "\n",
        "\n",
        "        #misc\n",
        "        colors = soup.select_one('[data-spec=\"colors\"]').get_text().split(\",\") if soup.select_one('[data-spec=\"colors\"]') else 'Unknown'\n",
        "        models = soup.select_one('[data-spec=\"models\"]').get_text().split(\",\") if soup.select_one('[data-spec=\"models\"]') else \"Unknown\"\n",
        "\n",
        "        price_element = soup.find('td', {'data-spec': 'price'})\n",
        "        price_text = price_element.get_text(strip=True) if price_element else 'Unknown'\n",
        "        prices = {}\n",
        "        if 'About' in price_text:\n",
        "            prices['About'] = ''.join(filter(lambda x: x.isdigit() or x == '.', price_text.replace(\"About\", \"\").strip()))\n",
        "        else:\n",
        "            for part in price_text.split('/'):\n",
        "                part = part.strip()\n",
        "                if '$' in part:\n",
        "                    prices['USD'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"$\", \"\").strip()))\n",
        "                elif '€' in part:\n",
        "                    prices['EUR'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"€\", \"\").strip()))\n",
        "                elif '£' in part:\n",
        "                    prices['GBP'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"£\", \"\").strip()))\n",
        "                elif '₹' in part:\n",
        "                    prices['INR'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"₹\", \"\").strip()))\n",
        "\n",
        "        #lable\n",
        "        label = 0\n",
        "        watch_keywords = ['watch', 'smartwatch']\n",
        "        if any(keyword in full_url.lower() for keyword in watch_keywords):\n",
        "            label = 1\n",
        "\n",
        "        #######################################\n",
        "        # save on  to dict\n",
        "        phone_data={'url':full_url,\"name\":name,\"network\":network,\"announcedDate\":announcedDate,\"releaseDate\":releaseDate,\n",
        "        \"availability\":availability,\"dimensions\":dimensions,\"weight\":weight,\"build\":build,\"sim\":sim,\"displaytype\":display_type,\n",
        "        \"displaysizeI\":displaysizeI,\"displaysizeCM\":displaysizeCM,\"displayratio\":displayratio,\"resolution\":resolution,\"protection\":protection,\n",
        "        \"other_features\":other_features,\"ostype\":ostype,\"osversion\":osversion,\"chipset\":chipset,\"cpu\":cpu,\"gpu\":gpu,\"cardslot\":card_slot,\n",
        "        \"internal_memory\":internal_memory,\"memory_other\":memory_other,\"mainCam\":mainCam,\"mainCamNum\":mainCamNum,\"mainCamFeatures\":mainCamFeatures,\n",
        "        \"mainCamVideo\":mainCamVideo,\"selfieCam\":selfieCam,\"selfieCamNum\":selfieCamNum,\"selfieCamFeatures\":selfieCamFeatures,\n",
        "        \"selfieCamVideo\":selfieCamVideo,\"loudspeaker\":loudspeaker,\"jack_info_3.5mm\":jack,\"wlan\":wlan,\"bluetooth\":bluetooth,\"gps\":gps,\n",
        "        \"nfc\":nfc,\"radio\":radio,\"usb\":usb,\"sensors\":sensors,\"features_other\":features_other,\"batteryType\":batteryType,\"charging\":charging,\n",
        "        \"colors\":colors,\"models\":models,\"price\":prices,'lable':label}\n",
        "        #######################################\n",
        "        return phone_data\n",
        "    else:\n",
        "        return {'error': 'Failed to fetch phone data because of response is not 200', 'url': full_url}\n"
      ],
      "metadata": {
        "id": "QV7LwWi2ghfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get url for each phone from the <img> tag\n",
        "\n",
        "def phone_href(brand_url: str):\n",
        "    phones_href = []\n",
        "    next_page_url = f\"{BASE_URL}/{brand_url}\"\n",
        "    while next_page_url:\n",
        "        response = make_request(next_page_url, HEADERS)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            phones = soup.select('.makers ul li a')\n",
        "            year_before_2010_found = False\n",
        "            for phone in phones:\n",
        "                time.sleep(1)\n",
        "                img_tag = phone.find('img')\n",
        "                if img_tag and 'title' in img_tag.attrs:\n",
        "                    title_text = img_tag['title']\n",
        "                    if 'Announced' in title_text:\n",
        "                        year_announced = int(title_text.split('Announced ')[1].split('.')[0].split(' ')[-1])\n",
        "                    else:\n",
        "                        print(f\"Announcement year not found in title: {title_text}\")\n",
        "\n",
        "                    if year_announced >= 2010:\n",
        "                        phones_href.append(phone['href'])\n",
        "                    else:\n",
        "                        year_before_2010_found = True\n",
        "                        break\n",
        "            if year_before_2010_found:\n",
        "                print(\"Found a phone announced before 2010, stopping.\")\n",
        "                break\n",
        "\n",
        "            next_page_link = soup.select_one('.nav-pages a.prevnextbutton[title=\"Next page\"]')\n",
        "            if next_page_link and 'href' in next_page_link.attrs:\n",
        "                next_page_url = f\"{BASE_URL}/{next_page_link['href']}\"\n",
        "            else:\n",
        "                print('not more pages')\n",
        "                next_page_url = None\n",
        "        else:\n",
        "            print(f\"Failed to fetch page: {next_page_url}\")\n",
        "            break\n",
        "\n",
        "    return phones_href\n"
      ],
      "metadata": {
        "id": "_kwOFbFmgKzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get brands urls\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    home_page_response = make_request(BASE_URL, HEADERS)\n",
        "    if home_page_response.status_code == 200:\n",
        "        soup = BeautifulSoup(home_page_response.content, 'html.parser')\n",
        "        brands = get_all_brands(soup)\n",
        "        print(brands)\n",
        "    else:\n",
        "        print(f'the response is : {home_page_response.status_code}')"
      ],
      "metadata": {
        "id": "kzE5OMPfg5L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 ways to collect data"
      ],
      "metadata": {
        "id": "lpFsf58DkouX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### first way : collect data for each brands is good idea"
      ],
      "metadata": {
        "id": "pyNPsIs6k6VA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get phones url for each brand\n",
        "brand = 'NOKIA'\n",
        "path = brands[brand]\n",
        "print(f\"Fetching phones for {brand}...\")\n",
        "phones_href = phone_href(path)\n",
        "print(len(phones_href))\n",
        "for phone in phones_href:\n",
        "    print(phone)"
      ],
      "metadata": {
        "id": "SfD1dLVRg57E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_phone_data_NOKIA = []\n",
        "\n",
        "for phone_url in phones_href:\n",
        "    print(phone_url)\n",
        "    phone_data = fetch_phone_data(phone_url)\n",
        "    if 'error' not in phone_data:\n",
        "        all_phone_data_NOKIA.append(phone_data)\n",
        "    else:\n",
        "        print(f\"Error fetching data for URL: {phone_url}\")\n"
      ],
      "metadata": {
        "id": "1qBLKsZElNRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phones_df_NOKIA = pd.DataFrame(all_phone_data_NOKIA)\n",
        "\n",
        "watch_keywords = ['watch', 'smartwatch', 'fit']\n",
        "tablet_keywords = ['tab', 'tablet']\n",
        "phone_keywords = watch_keywords + tablet_keywords\n",
        "\n",
        "df_watch  = phones_df_NOKIA[ phones_df_NOKIA['url'].apply(lambda x: any(keyword in x.lower() for keyword in watch_keywords)) ]\n",
        "df_tablet = phones_df_NOKIA[ phones_df_NOKIA['url'].apply(lambda x: any(keyword in x.lower() for keyword in tablet_keywords)) ]\n",
        "df_phone  = phones_df_NOKIA[ ~phones_df_NOKIA['url'].apply(lambda x: any(keyword in x.lower() for keyword in phone_keywords)) ]\n",
        "df_totall = phones_df_NOKIA\n",
        "\n",
        "df_watch.to_excel('NOKIA_watch.xlsx')\n",
        "df_tablet.to_excel('NOKIA_tablet.xlsx')\n",
        "df_phone.to_excel('NOKIA_phone.xlsx')\n",
        "df_totall.to_excel('NOKIA_product.xlsx')\n"
      ],
      "metadata": {
        "id": "fMR4KBOMiznb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### second way : collect whole data"
      ],
      "metadata": {
        "id": "AqSajuftlAsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_phone_data = []  # List to hold data for all phones across all brands\n",
        "\n",
        "for brand, path in brands.items():\n",
        "    print(f\"Fetching phones for {brand}...\")\n",
        "    phones_href = phone_href(path)\n",
        "\n",
        "    for phone_url in phones_href:\n",
        "        phone_data = fetch_phone_data(phone_url)\n",
        "        if 'error' not in phone_data:\n",
        "            all_phone_data.append({**phone_data, 'brand': brand})\n",
        "        else:\n",
        "            print(f\"Error fetching data for URL: {phone_url}\")\n",
        "\n",
        "# Convert the list of all phone data into a DataFrame\n",
        "phones_df = pd.DataFrame(all_phone_data)"
      ],
      "metadata": {
        "id": "rt-n_wUslFct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### theread"
      ],
      "metadata": {
        "id": "mu5xNimu1QLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "from queue import Queue\n",
        "\n",
        "phones_data_queue = Queue()\n",
        "\n",
        "def thread_fetch_phone_data(phone_urls):\n",
        "    for phone_url in phone_urls:\n",
        "        phone_data = fetch_phone_data(phone_url)\n",
        "        if 'error' not in phone_data:\n",
        "            phones_data_queue.put(phone_data)\n",
        "        else:\n",
        "            print(f\"Error fetching data for URL: {phone_url}\")\n",
        "\n",
        "def fetch_data_with_two_threads(phones_urls):\n",
        "    # Split the URLs into two parts >> use 2 thereads\n",
        "    midpoint = len(phones_urls) // 2\n",
        "    first_half_urls = phones_urls[:midpoint]\n",
        "    second_half_urls = phones_urls[midpoint:]\n",
        "\n",
        "    # Create two threads\n",
        "    thread1 = threading.Thread(target=thread_fetch_phone_data, args=(first_half_urls,))\n",
        "    thread2 = threading.Thread(target=thread_fetch_phone_data, args=(second_half_urls,))\n",
        "\n",
        "    # Start the threads\n",
        "    thread1.start()\n",
        "    thread2.start()\n",
        "\n",
        "    # Wait for both threads to complete\n",
        "    thread1.join()\n",
        "    thread2.join()\n",
        "\n",
        "    # Extracting data from the queue\n",
        "    all_phone_data = []\n",
        "    while not phones_data_queue.empty():\n",
        "        all_phone_data.append(phones_data_queue.get())\n",
        "\n",
        "    return all_phone_data\n",
        "\n",
        "# Use this function instead of the loop to fetch phone data\n",
        "all_phone_data_NOKIA = fetch_data_with_two_threads(phones_href)\n",
        "\n",
        "\n",
        "phones_df_NOKIA = pd.DataFrame(all_phone_data_NOKIA)\n",
        "\n",
        "watch_keywords = ['watch', 'smartwatch', 'fit']\n",
        "tablet_keywords = ['tab', 'tablet']\n",
        "phone_keywords = watch_keywords + tablet_keywords\n",
        "\n",
        "df_watch  = phones_df_NOKIA[ phones_df_NOKIA['url'].apply(lambda x: any(keyword in x.lower() for keyword in watch_keywords)) ]\n",
        "df_tablet = phones_df_NOKIA[ phones_df_NOKIA['url'].apply(lambda x: any(keyword in x.lower() for keyword in tablet_keywords)) ]\n",
        "df_phone  = phones_df_NOKIA[ ~phones_df_NOKIA['url'].apply(lambda x: any(keyword in x.lower() for keyword in phone_keywords)) ]\n",
        "df_totall = phones_df_NOKIA\n",
        "\n",
        "df_watch.to_excel('NOKIA_watch.xlsx')\n",
        "df_tablet.to_excel('NOKIA_tablet.xlsx')\n",
        "df_phone.to_excel('NOKIA_phone.xlsx')\n",
        "df_totall.to_excel('NOKIA_product.xlsx')"
      ],
      "metadata": {
        "id": "-W0j27Dj1Sza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extract with proxy"
      ],
      "metadata": {
        "id": "-vJcfw-zybQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_proxy = 'https://www.us-proxy.org/'\n",
        "ip_addresses = ['155.94.241.133' ,\n",
        "'5.161.103.41:88' ,\n",
        "'104.236.195.90:10009' ,\n",
        "'172.173.132.85:80' ,\n",
        "'93.188.161.84:80',\n",
        "'68.183.48.146:10006',\n",
        "'143.198.226.25:80',\n",
        "'198.176.56.42:80',\n",
        "'31.220.56.210:80',\n",
        "'155.94.241.134:3128',\n",
        "'138.68.235.51:80',\n",
        "'202.5.16.44:80',\n",
        "'155.94.241.132:3128',\n",
        "'155.94.241.130:3128',\n",
        "'72.169.67.61:87',\n",
        "'34.122.187.196:80',\n",
        "'198.176.56.39:80',\n",
        "'104.45.128.122:80',\n",
        "'216.137.184.253:80',\n",
        "'198.176.56.43:80']"
      ],
      "metadata": {
        "id": "Z0qng3IFyXJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 way to connect it\n",
        "\n",
        "\n",
        "def proxy_request(url, headers, ip_addresses):\n",
        "    while True:\n",
        "        try:\n",
        "            proxy = random.randint(0, len(ip_addresses) - 1)\n",
        "            print(f'which proxy?:{proxy}')\n",
        "            proxies = {\"http\": ip_addresses[proxy]}\n",
        "            response = requests.get(url, proxies=proxies, timeout=25, headers=headers)\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            else:\n",
        "                print(f\"Failed to fetch page: {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error while making a request: {str(e)}\")\n",
        "            pass\n",
        "\n",
        "'''\n",
        "def proxy_request(url, headers, ip_addresses):\n",
        "    backoff_time = 2\n",
        "    max_try = 3\n",
        "\n",
        "    for _ in range(max_try):\n",
        "        try:\n",
        "            proxy = random.choice(ip_addresses)\n",
        "            proxies = {\"http\": f\"http://{proxy}\"}\n",
        "            response = requests.get(url, proxies=proxies, timeout=25, headers=HEADERS)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                print(f\"Rate limited. Waiting for {backoff_time} seconds.\")\n",
        "                time.sleep(backoff_time)\n",
        "                backoff_time *= 2\n",
        "            else:\n",
        "                raise\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error while making a request: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    return None\n",
        "'''"
      ],
      "metadata": {
        "id": "MCVqGHIByZ-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#clean data"
      ],
      "metadata": {
        "id": "BQrJ3QqTj3m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data and concate this\n",
        "\n",
        "df_sumsung = pd.read_excel('/content/Samsung_product.xlsx')\n",
        "df_apple = pd.read_excel('/content/APPLE_product.xlsx')\n",
        "df_huawei = pd.read_excel('/content/Huawei_product.xlsx')\n",
        "df_sony = pd.read_excel('/content/SONY_product.xlsx')\n",
        "df_lg = pd.read_excel('/content/LG_product.xlsx')\n",
        "df_htc = pd.read_excel('/content/HTC_product.xlsx')\n",
        "df_lenovo = pd.read_excel('/content/LENOVO_product.xlsx')\n",
        "df_xiaomi = pd.read_excel('/content/XIAOMI_product.xlsx')\n",
        "df_asus = pd.read_excel('/content/ASUS_product.xlsx')\n",
        "df_alcatel = pd.read_excel('/content/ALCATEL_product.xlsx')\n",
        "df_zte = pd.read_excel('/content/ZTE_product.xlsx')\n",
        "df_infinix =  pd.read_excel('/content/INFINIX_product.xlsx')\n",
        "df_nokia = pd.read_excel('/content/NOKIA_product.xlsx')"
      ],
      "metadata": {
        "id": "tKRXBmLJj5yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'df_sumsung:{len(df_sumsung)}')\n",
        "print(f'df_apple:{len(df_apple)}')\n",
        "print(f'df_huawei:{len(df_huawei)}')\n",
        "print(f'df_sony:{len(df_sony)}')\n",
        "print(f'df_lg:{len(df_lg)}')\n",
        "print(f'df_htc:{len(df_htc)}')\n",
        "print(f'df_lenovo:{len(df_lenovo)}')\n",
        "print(f'df_xiaomi:{len(df_xiaomi)}')\n",
        "print(f'df_asus:{len(df_asus)}')\n",
        "print(f'df_alcatel:{len(df_alcatel)}')\n",
        "print(f'df_zte:{len(df_zte)}')\n",
        "print(f'df_infinix:{len(df_infinix)}')\n",
        "print(f'df_nokia:{len(df_nokia)}')"
      ],
      "metadata": {
        "id": "2S4nOaXSl2M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_sumsung, df_apple, df_huawei, df_sony, df_lg, df_htc, df_lenovo,\n",
        "                            df_xiaomi, df_asus, df_alcatel, df_zte, df_infinix, df_nokia])\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "o2xgSKgDl4x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_excel('products.xlsx')"
      ],
      "metadata": {
        "id": "72f2zOxol-Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cheack announce data( try to retry fill Unknown values)"
      ],
      "metadata": {
        "id": "D_NOj47vmKuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.copy()"
      ],
      "metadata": {
        "id": "5aMYhuromtZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL: str = 'https://www.gsmarena.com'\n",
        "\n",
        "HEADERS: Dict[str, str] = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 '\n",
        "                  'Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,'\n",
        "              'application/signed-exchange;v=b3;q=0.7',\n",
        "    'Accept-Language': 'en-US,en;q=0.9'\n",
        "}\n",
        "\n",
        "def make_request(url:str, headers:Dict[str, str]) -> requests.Response:\n",
        "    try:\n",
        "        time.sleep(3)\n",
        "        response = requests.get(url, timeout=50, headers=headers)\n",
        "        response.raise_for_status()  # Raises a HTTPError if the response status code is 4XX/5XX\n",
        "        return response\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error while making a request: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def Fill_announce_data(url: str) -> str:\n",
        "    response = make_request(url, HEADERS)\n",
        "    if response:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        announced_text = soup.select_one('[data-spec=\"year\"]').text if soup.select_one('[data-spec=\"year\"]') else None\n",
        "        if announced_text:\n",
        "            match = re.search(r'(20\\d{2})', announced_text)\n",
        "            if match:\n",
        "                year = match.group(1)\n",
        "                announcedDate = f'{year}-01'\n",
        "            else:\n",
        "                announcedDate = 'Unknown'\n",
        "        else:\n",
        "            announcedDate = 'Unknown'\n",
        "        return announcedDate\n",
        "    else:\n",
        "        return 'respone is not 200'\n",
        "\n",
        "for i, row in df2.iterrows():\n",
        "    if row['announcedDate'] == 'Unknown':\n",
        "        Date = Fill_announce_data(row['url'])\n",
        "        df2.loc[i, 'announcedDate'] = Date"
      ],
      "metadata": {
        "id": "Cp5895ozmJpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to decrease or not ? >> yes decrease\n",
        "df2.announcedDate.isin(['Unknown']).value_counts()\n"
      ],
      "metadata": {
        "id": "6vaP3Dozmx3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## set brand label"
      ],
      "metadata": {
        "id": "KS6OBSR6mmDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i , row in df2.iterrows() :\n",
        "    if 'apple' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Apple'\n",
        "    if 'alcatel' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Alcatel'\n",
        "    if 'asus' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Asus'\n",
        "    if 'htc' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Htc'\n",
        "    if 'huawei' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Huawei'\n",
        "    if 'infinix' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Infinix'\n",
        "    if 'lenovo' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Lenovo'\n",
        "    if 'lg' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Lg'\n",
        "    if 'nokia' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Nokia'\n",
        "    if 'samsung' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Samsung'\n",
        "    if 'sony' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Sony'\n",
        "    if 'xiaomi' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Xiaomi'\n",
        "    if 'zte' in row['url'].lower():\n",
        "        df2.loc[i, 'brand_label'] = 'Zte'"
      ],
      "metadata": {
        "id": "t5dSPekXmva0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.brand_label.value_counts()"
      ],
      "metadata": {
        "id": "tBDjS29mnExX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  set  product label"
      ],
      "metadata": {
        "id": "N6wM8oZ7nLv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tablet_keywords = ['pad', 'ipad' , 'tablet' , 'air' , 'tab']\n",
        "watch_keywords = ['watch' , 'smartwatch' , 'smart_watch' , 'smaty-watch' , 'fix' ]\n",
        "mobile_words = tablet_keywords + watch_keywords\n",
        "\n",
        "for index, row in df2.iterrows():\n",
        "    product_label = 'Unknown'\n",
        "\n",
        "    if any(keyword in row['url'].lower() for keyword in tablet_keywords):\n",
        "        product_label = 'Tablet'\n",
        "\n",
        "    elif any(keyword in row['url'].lower() for keyword in watch_keywords):\n",
        "        product_label = 'Watch'\n",
        "\n",
        "    else:\n",
        "        product_label = 'Phone'\n",
        "\n",
        "    # Assign the product label to the current row\n",
        "    df2.loc[index, 'product_label'] = product_label\n"
      ],
      "metadata": {
        "id": "P8BNGyfvnPCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for nokia brands there are some mistake >> but ok !!"
      ],
      "metadata": {
        "id": "Q7jlXebTnXON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Released data ( check from some resources )"
      ],
      "metadata": {
        "id": "JGRBi5nBnnRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.releaseDate.isin(['Unknown']).value_counts()"
      ],
      "metadata": {
        "id": "BpEtBkB7nsj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.copy()"
      ],
      "metadata": {
        "id": "_MzDcMCmnvjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEADERS: Dict[str, str] = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 '\n",
        "                  'Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,'\n",
        "              'application/signed-exchange;v=b3;q=0.7',\n",
        "    'Accept-Language': 'en-US,en;q=0.9'\n",
        "}\n",
        "\n",
        "def make_request(url:str, headers:Dict[str, str]) -> requests.Response:\n",
        "    try:\n",
        "        time.sleep(1)\n",
        "        response = requests.get(url, timeout=50, headers=headers)\n",
        "        response.raise_for_status()  # Raises a HTTPError if the response status code is 4XX/5XX\n",
        "        return response\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error while making a request: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def Fill_Release_data(url: str) -> str:\n",
        "    response = make_request(url, HEADERS)\n",
        "    if response:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        release_pic = soup.select_one('[data-spec=\"released-hl\"]')\n",
        "\n",
        "        if release_pic:\n",
        "            return release_pic.text\n",
        "        else :\n",
        "            return 'Unknown'\n",
        "\n",
        "        #status_text = soup.find('td', {'data-spec': 'status'}).text if soup.find('td', {'data-spec': 'status'}) else None\n",
        "        #if status_text and \"Released\" in status_text:\n",
        "        #    return status_text\n",
        "\n",
        "\n",
        "        #announced_text = soup.select_one('[data-spec=\"year\"]').text if soup.select_one('[data-spec=\"year\"]') else None\n",
        "        #if announced_text and \"Released\" in announced_text:\n",
        "        #    return announced_text\n",
        "\n",
        "\n",
        "        #return 'Unknown'\n",
        "\n",
        "    else:\n",
        "        return 'response is not 200'\n",
        "\n",
        "\n",
        "for i, row in df3.iterrows():\n",
        "    print(i)\n",
        "    if row['releaseDate'] in ['Unknown','response is not 200']:\n",
        "        re_Date = Fill_Release_data(row['url'])\n",
        "        df3.loc[i, 'releaseDate'] = re_Date"
      ],
      "metadata": {
        "id": "eAzZe4Djnhct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.releaseDate.isin(['Unknown', 'response']).value_counts() # decrease to 1"
      ],
      "metadata": {
        "id": "zKufSUO0oaof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now time to convert in a unique format\n",
        "months = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
        "\n",
        "def releaase_data_check(value):\n",
        "    if 'Released' in value :\n",
        "        value = value.split('Released')[-1]\n",
        "        print(value)\n",
        "\n",
        "    if re.search(r'(\\d{4})-(\\d{2})', value):\n",
        "        return value\n",
        "\n",
        "    match1 = re.search(r'(\\d{4}\\d)', value)\n",
        "    if match1:\n",
        "        value = f'{match1.groups()}-01'\n",
        "        return value\n",
        "\n",
        "    match2 = re.search(r'(\\d{4}), (\\w+)', value)\n",
        "    year, month = match2.groups()\n",
        "    if match2 and month in months :\n",
        "        value = datetime.strptime(f'{year} {month}', '%Y %B').strftime('%Y-%m')\n",
        "        return value\n",
        "\n",
        "    if match2 and month not in months :\n",
        "        value = f'{year}-01'\n",
        "        return value\n",
        "\n",
        "    return 'not match'\n",
        "def release_data_check2(value):\n",
        "    if 'Released' in value:\n",
        "        value = value.split('Released')[-1].strip()\n",
        "        print(value)\n",
        "\n",
        "    match_correct = re.search(r'(\\d{4}-\\d{2})', value)\n",
        "    if match_correct:\n",
        "        return value\n",
        "\n",
        "    #matching the format YYYY-MM-DD or YYYY, Month\n",
        "    match_direct = re.search(r'(\\d{4}), (\\w+ \\d+|\\w+)', value)\n",
        "    if match_direct:\n",
        "        year, month_day = match_direct.groups()\n",
        "        month = month_day.split(' ')[0]  #'May 31 >> 'May'\n",
        "        if month in months:\n",
        "            try:\n",
        "                date_str = f\"{year} {month} 1\"\n",
        "                return datetime.strptime(date_str, '%Y %B %d').strftime('%Y-%m')\n",
        "            except ValueError:\n",
        "                return 'not match'\n",
        "\n",
        "    match_year_quarter = re.search(r'(\\d{4}), Q\\d', value)\n",
        "    if match_year_quarter:\n",
        "        year = match_year_quarter.group(1)\n",
        "        return f'{year}-01'\n",
        "\n",
        "    match_year = re.search(r'(\\d{4})', value)\n",
        "    if match_year:\n",
        "        year = match_year.group(1)\n",
        "        return f'{year}-01'\n",
        "\n",
        "    return 'Unknown'\n",
        "#print(release_data_check2('Released 2010, Q3'))\n",
        "#print(release_data_check2('Released 2022, May 31'))\n",
        "#print(release_data_check2('Released 2022, May Q3'))\n",
        "#print(release_data_check2('Released 2010, March'))\n",
        "#print(release_data_check2('2010-01'))\n",
        "#print(release_data_check2('2010-05'))\n",
        "\n"
      ],
      "metadata": {
        "id": "cYxF2ZTNod8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = df3.copy()\n",
        "df5['releaseDate'] = df3.releaseDate.apply(release_data_check)"
      ],
      "metadata": {
        "id": "wpbfshvtpilA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "df3 = df3.drop('brand_lable', axis=1)\n",
        "columns_list  = df3.columns.tolist()\n",
        "for i in columns_list:\n",
        "    unknown_count = df3[i].isin(['Unknown']).value_counts()\n",
        "    print(f'Number of unknown for {i} is: {unknown_count}')\n",
        "    print('//////////')\n",
        "'''\n",
        ""
      ],
      "metadata": {
        "id": "pKYRKrLJqUj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.to_excel('FINAL_PRODUCTS.xlsx')"
      ],
      "metadata": {
        "id": "O6-9p_8Pqct9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## core\n"
      ],
      "metadata": {
        "id": "PWHm_UXtq7xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FINAL_PRODUCTS = pd.read_excel('/content/FINAL_PRODUCTS.xlsx')"
      ],
      "metadata": {
        "id": "cwusSfhNrIlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FINAL_PRODUCTS2 = FINAL_PRODUCTS.copy()"
      ],
      "metadata": {
        "id": "bmuzI7ijrJcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hexa-core , Dual-core , Quad-core , Triple-code\n",
        "keywords = ['Hexa-core', 'Dual-core', 'Quad-core', 'Triple-core' , 'Deca-core' , 'Octa-core']\n",
        "\n",
        "\n",
        "for i, row in FINAL_PRODUCTS2.iterrows():\n",
        "    if row['cpu'] != 'Unknown':\n",
        "        for keyword in keywords:\n",
        "            if keyword in row['cpu']:\n",
        "                core = row['cpu'].split()[0].strip()\n",
        "                FINAL_PRODUCTS2.loc[i, 'cpu'] = core\n",
        "                break\n",
        "            else:\n",
        "                FINAL_PRODUCTS2.loc[i, 'cpu'] = 'Single_core'\n",
        "\n",
        "\n",
        "print(FINAL_PRODUCTS['cpu'].value_counts())\n",
        "print('///////////////////////')\n",
        "print(FINAL_PRODUCTS2['cpu'].value_counts())"
      ],
      "metadata": {
        "id": "fNZy9FvVrMrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FINAL_PRODUCTS2.to_excel('FINAL_PRODUCTS2.xlsx')"
      ],
      "metadata": {
        "id": "Wjsb5uGprS9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## price"
      ],
      "metadata": {
        "id": "Y43QosRcrvaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "totall = pd.read_excel('/content/FINAL_PRODUCTS2.xlsx')"
      ],
      "metadata": {
        "id": "YCIBO9Qorkoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total2 =  total1.copy()"
      ],
      "metadata": {
        "id": "9HozzSHwrzss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total1.price.isin(['Unknown',{},'']).value_counts()"
      ],
      "metadata": {
        "id": "lJvmJ9AgsGEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HEADERS: Dict[str, str] = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 '\n",
        "                  'Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,'\n",
        "              'application/signed-exchange;v=b3;q=0.7',\n",
        "    'Accept-Language': 'en-US,en;q=0.9'\n",
        "}\n",
        "\n",
        "def make_request(url:str, headers:Dict[str, str]) -> requests.Response:\n",
        "    try:\n",
        "        time.sleep(2)\n",
        "        response = requests.get(url, timeout=50, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        return response\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error while making a request: {str(e)}\")\n",
        "        return None\n",
        "'''\n",
        "def fill_price_data(url: str) -> str:\n",
        "    response = make_request(url, HEADERS)\n",
        "    if response:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        price_element = soup.find('td', {'data-spec': 'price'})\n",
        "        price_text = price_element.get_text(strip=True) if price_element else 'Unknown'\n",
        "\n",
        "        prices = {}\n",
        "        if 'About' in price_text:\n",
        "            prices['About'] = price_text\n",
        "        if '/' in price_text:\n",
        "            for part in price_text.split('/'):\n",
        "                part = part.strip()\n",
        "                if '$' in part:\n",
        "                    prices['USD'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"$\", \"\").strip()))\n",
        "                elif '€' in part:\n",
        "                    prices['EUR'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"€\", \"\").strip()))\n",
        "                elif '£' in part:\n",
        "                    prices['GBP'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"£\", \"\").strip()))\n",
        "                elif '₹' in part:\n",
        "                    prices['INR'] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(\"₹\", \"\").strip()))\n",
        "        else :\n",
        "            prices['Other'] = price_text\n",
        "\n",
        "        return prices\n",
        "    else:\n",
        "        return 'response is not 200'\n",
        "'''\n",
        "def fill_price_data(url: str) -> Dict[str, str]:\n",
        "    response = make_request(url, HEADERS)\n",
        "    if response:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        price_element = soup.find('td', {'data-spec': 'price'})\n",
        "        price_text = price_element.get_text(strip=True) if price_element else 'Unknown'\n",
        "\n",
        "        prices = {}\n",
        "        currency_symbols = {'$': 'USD', '€': 'EUR', '£': 'GBP', '₹': 'INR'}\n",
        "\n",
        "        if 'About' in price_text or '/' not in price_text:\n",
        "            for symbol, code in currency_symbols.items():\n",
        "                if symbol in price_text:\n",
        "                    prices[code] = ''.join(filter(lambda x: x.isdigit() or x == '.', price_text.replace(symbol, \"\").strip()))\n",
        "                    break\n",
        "            else:\n",
        "                prices['About'] = price_text\n",
        "\n",
        "        else:\n",
        "            for part in price_text.split('/'):\n",
        "                part = part.strip()\n",
        "                for symbol, code in currency_symbols.items():\n",
        "                    if symbol in part:\n",
        "                        prices[code] = ''.join(filter(lambda x: x.isdigit() or x == '.', part.replace(symbol, \"\").strip()))\n",
        "                        break\n",
        "        return prices\n",
        "\n",
        "\n",
        "    else:\n",
        "        return {'Error': 'Response is not 200 or request failed'}\n",
        "\n",
        "\n",
        "for i, row in total2.iterrows():\n",
        "    print(i)\n",
        "    re_price = fill_price_data(row['url'])\n",
        "    total2.loc[i, 'price'] = str(re_price)"
      ],
      "metadata": {
        "id": "Eomp3D5dsJZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total2.to_excel('total2.xlsx')"
      ],
      "metadata": {
        "id": "SlCr_nwstGJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toral2 = total2.copy()"
      ],
      "metadata": {
        "id": "ozFXufLvuz9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fix the price column values\n",
        "\n",
        "import ast  # To safely evaluate string literal to dictionary\n",
        "\n",
        "def Convert_to_eur(price_str):\n",
        "    # Default conversion rates\n",
        "    conversions = {'USD': 0.93, 'GBP': 1.17, 'INR': 0.011}\n",
        "\n",
        "    try:\n",
        "        # Convert the string to an actual dictionary\n",
        "        price_dict = ast.literal_eval(price_str)\n",
        "    except ValueError:\n",
        "        return 'Unknown'\n",
        "\n",
        "    if 'EUR' in price_dict:\n",
        "        return price_dict['EUR']\n",
        "\n",
        "    elif 'About' in price_dict:\n",
        "        about_text = price_dict['About']\n",
        "        # Extract the numeric value from the 'About' text\n",
        "        if 'EUR' in about_text:\n",
        "            # Convert the extracted amount directly to EUR\n",
        "            amount = about_text.split()[1]\n",
        "        elif 'INR' in about_text:\n",
        "            amount = float(about_text.split()[1]) * conversions['INR']\n",
        "        elif 'USD' in about_text:\n",
        "            amount = float(about_text.split()[1]) * conversions['USD']\n",
        "        elif 'GBP' in about_text:\n",
        "            amount = float(about_text.split()[1]) * conversions['GBP']\n",
        "        else:\n",
        "            return 'Unknown'\n",
        "\n",
        "        return \"{:.2f}\".format(float(amount))\n",
        "\n",
        "    else:\n",
        "        for currency, amount in price_dict.items():\n",
        "            if currency in conversions:\n",
        "                return \"{:.2f}\".format(float(amount) * conversions[currency])\n",
        "\n",
        "    return 'Unknown'\n",
        "\n",
        "# Example\n",
        "price_samples = [\n",
        "    \"{'EUR': '1299.00', 'USD': '1089.00', 'GBP': '1149.00', 'INR': '148900'}\",\n",
        "    \"{'About': 'About 160 EUR'}\",\n",
        "    \"{'USD': '909.97'}\",\n",
        "    \"{'About': 'Unknown'}\",\n",
        "    \"{'About': 'About 22000 INR'}\"]\n",
        "\n",
        "\n",
        "toral2['price_in_eur'] = toral2['price'].apply(Convert_to_eur)"
      ],
      "metadata": {
        "id": "K_Cl_UZ6savs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total2.price = toral2['price_in_eur']"
      ],
      "metadata": {
        "id": "DJYIqb2ZtlUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_products = total2.drop('lable' , axis=1)\n",
        "final_products.to_excel('FINAL_PRODUCTS2.xlsx')"
      ],
      "metadata": {
        "id": "IKwMkgvOwBPv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}